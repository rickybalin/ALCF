# Database config
database:
    # colocated, clustered
    launch: "colocated"
    port: 6780
    # Polaris: lo, hsn0
    # Cannot use lo for clustered
    network_interface: "lo"

# Run config
run_args:
    # any value
    nodes: 1
    # any value for clustered, 1 for colocated
    db_nNodes: 1
    # any value for clustered, 1 for colocated
    sim_nNodes: 1
    # machine specific, 32 for Polaris 48 for ThetaGPU
    cores_pn: 32
    # any number
    simprocs: 8
    # any number up to limits of CPU
    simprocs_pn: 8
    # any number
    mlprocs: 8
    # any number up to limits of CPU/GPU
    mlprocs_pn: 4
    # any number up to limits of CPU
    dbprocs_pn: 4
    # Polaris: none, core, list, numa
    sim_cpu_bind: "none"
    # Polaris: none, core, list, numa
    ml_cpu_bind: "none"

# Experiment config
experiment:
    name: "inference-example"
    # pbs, cobalt
    launcher: "pbs"

# Model Inference config
model:
    path: "./model_jit.pt"
    backend: "TORCH"
    # cpu, gpu
    device: "gpu"
    # 0 - simprocs for clustered
    # 0 - simprocs_pn for colocated
    batch: 0
    # 1 - number of GPUs available for inference
    # 1 if device=cpu
    devices_per_node: 1

# Simulation config
sim:
    executable: "./src/inference.py"
    # Device for simulation: cuda, cpu
    device: "cpu"

# Distributed training config
train:
    executable: ""
    # Device for simulation: cuda, cpu
    device: ""

# Logging config
# no, fom, verbose, verbose-perf. Set to fom for optimization
logging: "verbose-perf" 
